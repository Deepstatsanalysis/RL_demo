{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.  Reinforcement Learning\n",
    "\n",
    "This notebook introduces the fundamental Reinforcement Learning paradigm and algorithm in a deterministic setting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Common Reinforcement Learning problems\n",
    "\n",
    "To get a more intuitive sense of what these common components of Reinforcement Learning problems are, lets look at a variety of examples.\n",
    "\n",
    "### Example 1:  Grid world: making a shortest-path finding AI\n",
    "\n",
    "Grid world is a simplified version of what is called the shortest path problem - a problem often solved in video games (this is how enemy AI units find you in the game), mapping services (finding the shortest route from A to B), and robot path planning (e.g., for a cleaning robot) - get a user from its starting location to a desired destination in as few steps as possible.    \n",
    "\n",
    "There are many algorithms specifically designed to solve just this task - the most popular being [Dijkstraâ€™s and A* algorithms](http://www.redblobgames.com/pathfinding/a-star/introduction.html).  However here we will use the flexible RL framework as it too provides great results, and the (relative) simplicity of the problem will allow us to illustrate the more general RL learning process hopefully clearly.\n",
    "\n",
    "In this toy problem we have a grid like the one illustrated below.  Each square tile is a location in the world.  Here the black square denotes the user, the green square the desired destination, and the blue squares impenetrable obstacles (the user cannot move to).  \n",
    "\n",
    "The actions available to our user are to move one unit left, right, up, or down, or stay still, and it can move to any free square (here colored magenta) or the goal (colored green).  If the user ever moves to the goal square the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 0, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 255, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_gridworld import my_gridworld\n",
    "test = my_gridworld()\n",
    "test.color_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example 2:  Making an AI that can play chess\n",
    "\n",
    "Another problem RL can be applied to is making an intelligent agent to play (and win) the game of chess.\n",
    "\n",
    "<img src=\"images/chess.png\",width=250,height=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3:   An AI that can balanace a pole on a moving cart (a.k.a. the inverted pendulum)\n",
    "\n",
    "A classic toy control problem  - teach an agent how to balance a pole on a moving cart.  The pole is free to rotate about an axis on the cart, with the cart on a track so that it may be moved left and right - affecting the location of the pole.  The pole feels the force of Earth's gravity, and so if unbalanced will fall to the ground.  \n",
    "\n",
    "<img src=\"images/cartpole.png\",width=300,height=300>\n",
    "\n",
    "This problem is a prototype for more general problems in mechanical control and robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  The fundamental components of an RL problem\n",
    "\n",
    "Lets define the modeling components of a standard Reinforcement Learning problem - afterwards we will return to our example problems and describe the specific instances of these components for each example.  \n",
    "\n",
    "**There are *four* main components to the RL model: states, features, actions, and rewards.**  These are - at a high level:\n",
    "\n",
    "1\\.    A **state** is a specific configuration of all objects in a problem's enviroment.  A **state space** is the collection of all possible states this enviroment is capable of producing.  The state space is defined by the problem we aim to solve.\n",
    "\n",
    "- In *grid world* any state consists of one piece of information: the current location of the player (the black square).  The state space is all possible locations on the board where the player can go.\n",
    "\n",
    "\n",
    "- In the chess problem a state is any (legal) configuration of the white and black pieces on the board (and some pieces may be missing).  The state space is the set of all possible configurations of chess pieces on the board.  The state space is defined by the problem we aim to solve.\n",
    "\n",
    "\n",
    "- In the cart-pole problem a state is a complete set of information about the cart and pole's position.  This includes: the cart position, the cart velocity, the angle of the pole measured as its deviation from the vertical position, and the angular velocity of the pole.  While these are technically continuous values, in practice they are finelly discretized.  The state does **not** include technical / physical information about the enviroment e.g., the fact that gravity exists, its precise force, etc.,\n",
    "\n",
    "\n",
    "2\\.  The agent can then take an **action** - the set of actions is either determined by us or by the problem environment.  The **action space** is the total set of such available actions.  \n",
    "\n",
    "\n",
    "- For grid world, a toy problem, we can choose a set of actions.  For example we could decide that the agent can only move the black square adjacently one unit up/down/left/right or keep it still.  Alternatively, we could allow the agent to move the black square diagonally as well.\n",
    "\n",
    "\n",
    "- In the chess problem an action is a legal move of any of its current pieces on the board.\n",
    "\n",
    "\n",
    "- In the cart pole problem - as in robotics applications as well - the range of actions is completely defined by the available range of motions of the machine being directly controlled.  In cart pole the agent directly controls the cart, which in 2-dimensions can only move left or right.\n",
    "\n",
    "\n",
    "3\\.  Once this action is taken the agent receives a **reward** - or a signal that the agent has performed a good or bad action based on our desired goal.  We (humans) decide on the reward structure (in order to induce our agent to accomplish our desired goal).  We want a reward for a given action to be **larger** for those actions that get us closer to accomplishing our goal (and less for those actions which do not), or vice versa.  To reiterate: this is the (only) way we communicate our desired goal to the agent.\n",
    "\n",
    "- For grid world we can assign a negative value like -1 to all actions (one unit movement) which leads to a non-goal state, and a large positive number like 1000 to actions leading to the **goal state** itself.  However these exact numbers are arbitrary, so long as a negative reward is given to actions not directly leading to the goal (and a positive reward is given when the goal square is reached).\n",
    " \n",
    " \n",
    "- For the chess problem one reward structure to induce our agent to learn how to win could be as follows: any move made that does not lead to the **goal state** (checkmating the opponent) recieves -1 reward, a move that sucessfully check mates the oponnent receieves a reward of 10000\n",
    "\n",
    "\n",
    "- For the cart-pole problem one common choice of reward structure - at every state at which the angle of the pole is above a certain threshold the reward is 1, otherwise it is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.3  How the four RL pieces fit together\n",
    "\n",
    "These four pieces connect together to create a feedback system that - when properly engineered - allow us to train an AI agent to accomplish a desired goal via maximization of the surrogate rewards.  In order to derive the RL algorithms that allow for this we need to first represent each of the components algebraically.\n",
    "\n",
    "### Problem solving in steps and RL component notation\n",
    "\n",
    "For now suppose a for a given problem that there are only $N$ possible states and $M$ available actions.  \n",
    "\n",
    "- In the grid world problem the number of states is finite and equal to the number of places the player can travel.\n",
    "\n",
    "\n",
    "- In the chess problem the number of states is equal to the total number of possible configurations of the board, while the number of actions is equal to the number of legal moves available to the pieces on the board.\n",
    "\n",
    "\n",
    "- In the cart pole problem the number of states is equal to the number of configurations of the 4 discreteized enviromental descriptors, and there are only three actions: move the cart left, right, or keep it still\n",
    "\n",
    "\n",
    "For a given problem we will denote the state and actions spaces as follows\n",
    "\n",
    "- Denote the set of all states of the system  $S=\\left\\{ \\sigma_{1},\\sigma_{2},...,\\sigma_{N}\\right\\} $ \n",
    "\n",
    "\n",
    "- Denote the set of all actions available  $A=\\left\\{ \\alpha_{1},\\alpha_{2},...,\\alpha_{M}\\right\\} $\n",
    "\n",
    "\n",
    "Notice that if we ourselves were to solve any of the examples given so far we would do so sequentially, in discrete steps.  This is precisely how the agent solves these problems as well.  \n",
    "\n",
    "- In grid world we / the agent move the player one square at a time.  Beginning at a state (location) an action is taken to move the player to a new state.  For taking this action and moving to the new state the agent receives a reward.  This reward is either positive or negative based on whether - in the long run - this helped or hindered it achieivng the goal (reaching the green square).\n",
    "\n",
    "\n",
    "- In chess we / the agent moves one piece at a time.  Beginning at a state (a specific configuration of the board) an action is taken and a piece on the board is moved, a new state of the system.  For taking this action and moving to the new state the agent receives a reward.  This reward is either positive or negative based on whether - in the long run - this helped or hindered it achieivng the goal (check mating the opponent).\n",
    "\n",
    "\n",
    "- In cart pole we / the agent keep the cart still or move it with a fixed force left or right.  Beginning at a state (a specific configuration of the 4 system descriptors) an action is taken and the cart is moved, a new state of the system arises.  For taking this action and moving to the new state the agent receives a reward.  This reward is either positive or negative based on whether - in the long run - this helped or hindered it achieivng the goal (keeping the pole in the air as long as possible).\n",
    "\n",
    "\n",
    "Some notation to describe what occurs at the $k^{th}$ step in solving a problem: at this step an agent begins at a state $s_{k-1} \\in S$, takes an action $a_k \\in A$ that moves the system to a state $s_k \\in S$.\n",
    "\n",
    "Notice too a key part of each of these descriptions when it comes to an agent solving a given problem: the mechanism by which an agent learns the best action to take in a given state is the reward it recieves.  This reward is positive or negative based on how each action helps or hinders its accomplishment of the goal.\n",
    "\n",
    "We need notation for the reward an agent recieves at the $k^{th}$ step as well: call this $r_k$.  This is a function of the initial state at this step the action taken - i.e., $r_k = r_k(s_{k-1},a_k)$  \n",
    "\n",
    "(Note: the problems considered here are deterministic in nature.  That is if we take action $a_k$ at state $s_{k-1}$ we always reach the same preceeding state $s_k$.  In problems that are stochastic in nature - where an action $a_k$ at $s_{k-1}$ does not always lead to the same state $s_k$ the reward function must also necessarily be a function of $s_k$ as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodes of simulation\n",
    "\n",
    "<img src=\"images/RL_graph.png\",width=300,height=300>\n",
    "\n",
    "So in solving a problem an agent goes through a sequence of events at each step.  These are: start at a state, take an action, move to a new state, and recieve a corresponding reward.  The first three steps look like\n",
    "\n",
    "**step 1:** start at state 0 $s_0$ --> take action 1 $a_1$ --> move to state 1 $s_1$ + recieve reward 1 $r_1$\n",
    "\n",
    "**step 2:** start at state 1 $s_1$ --> take action 2 $a_2$--> move to state 2 $s_2$ + recieve reward 2 $r_2$\n",
    "\n",
    "**step 3:** start at state 2 $s_2$ --> take action 3 $a_3$ --> move to state 3 $s_3$ + receive reward 3 $r_3$\n",
    "\n",
    "**step 4:** ...\n",
    "\n",
    "or in short the first three steps look like\n",
    "\n",
    "($s_0$, $a_1$, $r_1$), ($s_1$, $a_2$, $r_2$), ($s_2$, $a_3$, $r_3$), ($s_3$,...\n",
    "\n",
    "Taken together a sequence of such steps - ending either when a goal state is reached (as in the grid world or chess examples) or after a maximum number of iterations is completed (as in the cart pole example) - is referred to in RL jargon as an **episode**.  \n",
    "\n",
    "What do we want the perfect epsiode of steps to look like when performed by a trained agent?  We want the agent to choose actions in order to maximize its long term reward - that is the sum of rewards over all the steps in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Training the agent by Q learning\n",
    "\n",
    "So far we have set up the standard RL problems, defined its various components and introduced notation for each, and discussed how these components interact in each step of a problem solving process.  In this Section we will discuss a basic approach to training an agent to take the right actions to maximize its total rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  The fundamental and recursive RL cost function\n",
    "\n",
    "How do we do this - train the agent to take the proper actions at each state to maximize its total reward?  First - as with all machine learning problems - we need to form a cost function.  \n",
    "\n",
    "Lets take an arbitrary episode of steps beginning at state $s_0$ where we take action $a_1$\n",
    "\n",
    "($s_0$, $a_1$, $r_1$), ($s_1$, $a_2$, $r_2$), ($s_2$, $a_3$, $r_3$), ($s_3$,...\n",
    "\n",
    "After $a_1$ is taken want to determine the best sequence of remaining actions so that the sum of the rewards for these actions is as large as possible.\n",
    "\n",
    "So denote by $Q(s_0,a_1)$ the *maximum total reward possible* if we begin at a state $s_0$, take the action $a_1$ bringing us to some state $s_1$, and then continue taking steps until goal is reached or a maximum number of steps is taken.  By definition this quantity is the sum of the realized reward $r_1$ (we recieve for taking action $a_1$ at state $s_0$) plus the largest possible rewards from all the preceeding steps.  \n",
    "\n",
    "The only thing that is not 'maximal' about this $Q$ is the first reward $r_1$ - since there could be another action that provides a larger first one.  So then the maximum possible value of $Q$ at the state $s_0$ is achieved when we choose this best action at state $s_0$ \n",
    "\n",
    "$\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{0},\\,a_{i}\\right)$\n",
    "\n",
    "With the above the $Q$ function can be defined *recursively* in terms of itself.  This is because as stated the quantity $Q(s_0,a_1)$ must be the sum of $r_1$ - the reward for taking the action $a_1$ at $s_0$ - plus the *maximum possible sum of rewards for all subsequent steps* $\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{1},\\,a_{i}\\right)$.\n",
    "\n",
    "Writing this out algebraically (often referred to as **Bellman's equation** this is precisely\n",
    "\n",
    "$Q\\left(s_{0},\\,a_{1}\\right)=r_{1}+\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{1},\\,a_{i}\\right)$\n",
    "\n",
    "And in fact this definition holds regardless of what state and action we begin with.  i.e., at the ${k-1}^th$ step $s_{k-1}$ if we take action $a_k$ we then have the relationship \n",
    "\n",
    "$Q\\left(s_{k-1},\\,a_{k}\\right)=r_{k}+\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Slight adjustment to dampen the effect of late actions\n",
    "\n",
    "Often in practice one includes a parameter$\\gamma \\in [0,1]$ to regularize or dampens later rewards and prevent the affect of later actions to interfere with maximal actions in the present state $s_0$.  Adding this parameter we have a recursive definition of $Q$ as\n",
    "\n",
    "$Q\\left(s_{k-1},\\,a_{k}\\right)=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$\n",
    "\n",
    "By scaling $\\gamma$ up and down we can lessen the contribution of future rewards.  For example\n",
    "\n",
    "- When we set $\\gamma = 0$ then only the first reward remains $r_1$.   Maximizing $Q$ means we maximize the reward given by the first step of the process, for all states.  Our agent learns to take a 'greedy' approach to accomplishing our goal, at each state taking the next step that maximizes the next step reward only.\n",
    "\n",
    "\n",
    "- When we set $\\gamma = 1$ then we all the $\\gamma$'s disappear and we have our original cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Representing the Q function as a 2-dimensional matrix\n",
    "\n",
    "While mathematically the input to $Q(s,a)$ can have many dimensions, being pairs of raw states and actions, the way we compute it via *Bellman's equation* / store it on a computer is via a 2-dimensional matrix representation.  \n",
    "\n",
    "If we order the set of all possible states, and denote them as $S=\\left\\{ \\sigma_{1},\\sigma_{2},...,\\sigma_{N}\\right\\} $, and do the same thing with the set of all possible actions $ A=\\left\\{ \\alpha_{1},\\alpha_{2},...,\\alpha_{M}\\right\\} $ (note: this is not an ordering of states / actions as they occur, but of all possible states / actions available to the system), then we represent $Q$ on as computer as the $N \\times M$ matrix\n",
    "\n",
    "\n",
    "$Q=\\left[\\begin{array}{cccc}\n",
    "Q\\left(\\sigma_{0},\\alpha_{0}\\right) & Q\\left(\\sigma_{0},\\alpha_{1}\\right) & \\cdots & Q\\left(\\sigma_{0},\\alpha_{M}\\right)\\\\\n",
    "Q\\left(\\sigma_{1},\\alpha_{0}\\right) & \\ddots &  & Q\\left(\\sigma_{1},\\alpha_{1}\\right)\\\\\n",
    "\\vdots &  & \\ddots & \\vdots\\\\\n",
    "Q\\left(\\sigma_{N},\\alpha_{0}\\right) &  &  & Q\\left(\\sigma_{N},\\alpha_{M}\\right)\n",
    "\\end{array}\\right]$\n",
    "\n",
    "In other words, we represent $Q$ as a matrix indexed by action along its columns and state along its rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4  Computing Q in practice\n",
    "\n",
    "How do practically compute the $Q$ function in practice using the recursive definition discussed above?  At first it might seem like this is impossible - a chicken and egg problem - how do we compute $Q$ if it is defined via itself?\n",
    "\n",
    "The answer: by running a large number of episodes and updating $Q$ via the recursive definition as we go along.  In the most basic appoach we run each episode we and a random initial state, a random action, and repeat taking steps until a goal state is reached or maximum number of steps is taken.  At the $k^{th}$ step we are at a state $s_{k-1}$ and take a random action $a_k$ and update $Q(s_{k-1},a_k)$ using the recursive formula.\n",
    "\n",
    "By running through a large number of episodes - and so through the gamat of all states and actions multiple - updating $Q$ at each step we essentially learn $Q$ by trial-and-error interactions with the enviroment.  How well our computations converge to the true $Q$ function depend on how well we sample the state / action spaces.\n",
    "\n",
    "------\n",
    "Initialize $Q$, choose value for $\\gamma \\in [0,1]$\n",
    "\n",
    "**for** e = 1...E (the maximum number of episodes)\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a random initial state $s_0$ and $k=0$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **while** end state OR maximum iteration count NOT reached\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;select a random action $a_k$ at our current state $s_{k-1}$ and move to $s_k$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;update $Q$ as \n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q\\left(s_{k-1},\\,a_{k}\\right)=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\longrightarrow k+1$\n",
    "\n",
    "-----\n",
    "\n",
    "This algorithm - called **Q-learning** - is performed for the grid world example in the following Python cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning process complete\n"
     ]
    }
   ],
   "source": [
    "# train q learning and lets examine the matrix as it develops\n",
    "test.qlearn(gamma = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  What is the best action per state given a learned Q function?\n",
    "\n",
    "Given $Q(s_k,a)$ learned properly, how should the agent move at each state?  This is implicit in the recursive definition of $Q$ itself.  Since the action maximizing long term reward maximizing reward at state $s_k$ is given by \n",
    "\n",
    "$\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$\n",
    "\n",
    "the action $a_k$ providing this maximimum is given as the argument of this maximum\n",
    "\n",
    "$a_{k}^{\\star}=\\underset{i=1...M}{\\text{argmax}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$\n",
    "\n",
    "This is often referred to as an **optimal policy** function.  \n",
    "\n",
    "So in sum - after learning $Q$ properly we can start at any initial state $s_0$ and use the optimal policy function to take actions that allow us to travel in a reward maximizing path of states until we reach the goal (or a maximum number of steps are taken).\n",
    "\n",
    "In the next Python cell we illustrate how to use the optimal policy function with the grid world example (where we have already learned the proper $Q$ matrix in a previous Python cell).  You can initialize the agent at any square on the board (except barrier locations in blue), and activating the cell will animate its path (using the optimal policy function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(0, 0, 0);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr><tr><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td><td style=\"width: 35px; height: 35px; border: 5px solid white; background-color: rgb(234, 123, 234);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "starting_location = [1,1]\n",
    "test.animate_movement(starting_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  The importance of features\n",
    "\n",
    "In detailing the pieces of a standard RL problem in the first Section we mentioned 3 components: states, actions, and rewards.  But there is an important $4^{th}$ component to an RL system we did not mention - features.  \n",
    "\n",
    "In this Section we describe this final RL component and weave features into formal description in Section 2 that led to the Q learning scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  What are features?\n",
    "\n",
    "Features, the $4^{th}$ RL component, play the same role in Reinforcement Learning as they do in machine learning more generally: they characterize input so that a proper relationship between input and output can be learned.\n",
    "\n",
    "4\\.    **Features** are pieces of information that we (humans) design and which characterize each state in the state space.  The agent uses the features from each state to make its decision on how to behave to achieve our goal.  Again, what sort of information to give to our agent **is completely up to us to decide.**  \n",
    "\n",
    "\n",
    "-  For grid world a fine choice of features are simply the raw state data itself - that is the tuple (the horizontal / vertical coordinates) describing the location of the player.  Another choice: two features 1) the distance from the player to the goal (one feature) and 2) the distance from player to the nearest wall. \n",
    "\n",
    "\n",
    "- For the chess problem a natural choice is one set of 64 features, the $i^{th}$ feature indicating which (if any) piece is occupying the $i^{th}$ square of the board (there are 64 squares in total, hence 64 features).  However there are other choices we could make as well.\n",
    "\n",
    "\n",
    "- For the cart-pole problem a natural set of features are just the four state descriptors (discretized properly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Formally integrating features into the RL framework\n",
    "\n",
    "Formally we can denote a set of features extracted from a state $s$ using function notation as $f(s)$.  Feature transformations in RL are often a set of abstract processing steps on the problem enviroment (as in the grid-world or chess examples) and need not solely be a function in the classical mathematical sense (i.e., $f (s) = \\text{sin}(s)$ ).  \n",
    "\n",
    "Also note - that typically $f(s)$ is **vector valued**.  For example, the natural choice of features in grid world is a tuple $f(s) = (~f_1(s),~f_2(s)~)$ where $f_1$ and $f_2$ denote the $x$ and $y$ coordinate of the player.  Likewise for the chess problem the natural set of features has length 64 i.e., $f(s) = (f_1(s),~f_2(s),...,~f_{64}(s)~)$ where the $i^{th}$ feature indicates which (if any) piece is currently in the $i^{th}$ square of the board.\n",
    "\n",
    "An AI agent - and the Q learning process - learns based on the features extracted at each state.  This means that in general the input to the $Q$ function are not states and actions as $Q(s,a)$, but features and actions $Q(\\text{f}(s),a)$.  $Q$ is still stored and computed as an $N \\times M$ matrix, but is the features from each state that are used as input instead of the raw state itself (unless the features are chosen to be the state itself).\n",
    "\n",
    "\n",
    "$Q=\\left[\\begin{array}{cccc}\n",
    "Q\\left(f\\left(\\sigma_{0}\\right),\\alpha_{0}\\right) & Q\\left(f\\left(\\sigma_{0}\\right),\\alpha_{1}\\right) & \\cdots & Q\\left(f\\left(\\sigma_{0}\\right),\\alpha_{M}\\right)\\\\\n",
    "Q\\left(f\\left(\\sigma_{1}\\right),,\\alpha_{0}\\right) & \\ddots &  & Q\\left(f\\left(\\sigma_{1}\\right),\\alpha_{1}\\right)\\\\\n",
    "\\vdots &  & \\ddots & \\vdots\\\\\n",
    "Q\\left(f\\left(\\sigma_{N}\\right),,\\alpha_{0}\\right) &  &  & Q\\left(f\\left(\\sigma_{N}\\right),\\alpha_{M}\\right)\n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  What features should you choose \n",
    "\n",
    "### Choosing features that provide the best information about your problem states\n",
    "\n",
    "Many machine learning problems require that you know a substantial amount of information about a given domain in order to make a proper choice of features.  You obviously do not want features that represent your data incorrectly, as if you use such features no algorithm on earth can help you solve your problem.  In two class classification - for example - if you choose features that cause your classes to completely intermix you won't be able to perform your task very well.\n",
    "\n",
    "This intuition carries over to the RL setting as well.  Generally speaking the same holds in the RL setting as well.  If you are working on a complex robotics problem your time spent engineering features could be substantial.  Analog to classification, in the RL setting you want features that sharply distinguish between *states* of your system.  If your features fail to accomplish this you will train a poorly performing agent.  And designing features that accomplish this goal can require serious reflection and work.  \n",
    "\n",
    "For some RL problems - however - choosing reasonable features is fairly intuitive.  In the toy problems we discussed here - for example - choosing features was straight forward (sometimes we just took the raw state itself as the feature).  In other game related problems the choice can be more intuitive as well - but the more you understand the game the better the features you can devise.\n",
    "\n",
    "### The information content of features vs cost of simulation\n",
    "\n",
    "With typical machine learning problems you don't want your features to misrepresent your data (the worst case scenario), but you also don't want the to be uninformative (a milder problem).  Uninformative features don't mess up your data - e.g., in the two class classification scenario they don't mix up your two classes - but **the less informative your features are the more data you need to determine your system's underlying rule**.\n",
    "\n",
    "The same holds for the RL scenario - with one important wrinkle: with RL problems you *create your own dataset* in learning the $Q$ function properly.  So you can get away with having fairly uninformative features if you have the computational power to compensate for them, and run a very large number of epislodes.  This is certainly possible with a problems like the chess or cart pole examples - we can simulate until the cows come home using e.g., a cloud server.  The cost of simulating is low.  This is not the case for many robotics applications however, where an episode must be enacted by a physical robot (hence limiting the number of episodes we can effectively run).\n",
    "\n",
    "\n",
    "### Choosing features based on a product or academic goal\n",
    "\n",
    "Another question to consider when choosing features for RL is: what are your intentions?  \n",
    "\n",
    "- Are they practical?  Do you want to produce a robot with some automatic control feature and sell it to paying customers?  Then you want invest heavily in resources to construct a set of features that will work quickly and effectively with your application.\n",
    "\n",
    "\n",
    "- Are they academic-focused?  Then your goal may be set by a curiousity of your own or the academic establishment.  One academic question investigated a lot these days is to find a set of features you can effectively apply to solving a large set of visual problems (e.g., convolutional networks for training Atari AI).  These features are not very informative, but because computer power and storage is so great these days we can make these features work by running a very large number of episodes to learn $Q$ properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Dealing with large state spaces\n",
    "\n",
    "Notice one major fundamental difference (in terms of the RL components of each) between the chess and cart-pole examples and the grid-world example discussed previously: the size of the state-space.  \n",
    "\n",
    "- With the chess example: the number of possible configurations of chess pieces on the board is enormous - i.e., the number of states or size of state space - is on the order of $10^{120}$.  That is far larger than the number of atoms in the known universe!\n",
    "\n",
    "\n",
    "- With the cart-pole example: each of the four descriptors of the enviroment is continuous (or highly discretized) - thus the number of states is infinite.  \n",
    "\n",
    "\n",
    "In both cases the (very large) size of the state space presents practical problems (while the size of the action space is not a problem).  Why?  Because rememver that in order to learn the $Q$ function we have to be able to cycle through *every possible state*.  Practically speaking then even a state space of size $10^{120}$ is far too large for us to be able to learn a representative $Q$ effectively.  \n",
    "\n",
    "Even more practically, remember that so far we have been storing the $Q$ function as a matrix of size $N \\times M$, where $N$ and $M$ are the sizes of the state and action spaces respectively.  But the size of $N$ is - for these two problems and for most real world problems (in automatic control and game AI) far too large store in memory!  \n",
    "\n",
    "In sum the problem here that most real RL problems the **size of the state space** makes computing and storing the $Q$ function/matrix impossible.  Some RL problems can also have issues with large sized action spaces, but this is typically not as problematic (as action spaces can be more easily discretized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1  Compactly representing state space of the $Q$ function\n",
    "\n",
    "We need a way to compactly represent *the vertical or state-space dimension* of $Q$ when we have a very large (and possibly infinite number) of states.  Instead of trying to compute and record every input/output pair - which is impossible once the input space is too large or infinite - we can try the next best thing: represent $Q$ as a *parameterized function* and simply fit it to the input/output data.\n",
    "\n",
    "Does the concept of \"fitting a parameterized function to a set of input/output data\" sound familiar?  Thats just a fancy way of saying that we'll use **regression** - a fundamental machine learning tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Using regression to represent the columns of Q\n",
    "\n",
    "Instead of storing the entire matrix $Q$, we will substitute a parameterized function for each of its columns - independently.   The $k^{th}$ available action $a_k$ has a corresponding slice of $Q$ (an $N \\times 1$ column vector)\n",
    "\n",
    "$Q_k=\\left[\\begin{array}{c}\n",
    "Q\\left(f(\\sigma_{0}),\\alpha_{k}\\right)\\\\\n",
    "Q\\left(f(\\sigma_{1}),\\alpha_{k}\\right)\\\\\n",
    "\\vdots\\\\\n",
    "Q\\left(f(\\sigma_{N}),\\alpha_{k}\\right)\n",
    "\\end{array}\\right]$\n",
    "\n",
    "As with the $Q$ matrix itself - the length of this vector is far too long to completely compute or store.  Note too that with the action fixed at $\\alpha_{k}$ every entry of this column is a single *point*, an input/output pair.  For example the $i^{th}$ point is $(f(\\sigma_{i}),Q\\left(f(\\sigma_{i}),\\alpha_{k}\\right))$.  \n",
    "\n",
    "So in other words, to represent the column of values using a parameterized function $h$ needs to be a function of the input here $f(s)$.  For example, suppose we have $P$ features - i.e., that  $f$ is a $P$ length vector $f(s) = (~f_1(s)~,f_2(s)~,...,f_P(s)~)$ then the simplest - yet still commonly used - parameterized representation is a linear combination of the input features\n",
    "\n",
    "\n",
    "$h_k(f(s);\\theta) = \\theta_{1}\\,f_{1}\\left(s\\right)+\\theta_{2}\\,f_{2}\\left(s\\right)+\\cdots+\\theta_{P}\\,f_{P}\\left(s\\right)$\n",
    "\n",
    "However more nonlinear parameterizations - like neural networks - are also commonly used.  \n",
    "\n",
    "Note that the hope here is that - when properly tuned - that this model accurately represents what would be the true $Q$ values as\n",
    "\n",
    "$h_k(f(s);\\theta) \\approx Q\\left(f(s),\\alpha_{k}\\right))$.\n",
    "\n",
    "While we now need to tune these parameters, the benefit is quite plain: instead of storing the original column vector we need only store the parameter values - e.g., in the case of a linear approximator we have only $P$ values. \n",
    "\n",
    "Finally note that since $Q$ originally has $M$ columns and we are replacing each with a parameterized model, we will $M$ total parameterized models representing $Q$.  These models are almost always taken to have the same form (i.e., all are either linear, neural networks, kernels, etc.,) so we can distinguish them via their index $h_0(f(s);\\theta_0)~,h_1(f(s);\\theta_1),\\cdots~h_M(f(s);\\theta_M)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Integrating regression into the Q-learning process\n",
    "\n",
    "Integrating this regression scheme into the Q-learning process requires we add one extra step to the original.  The original process has us - in a single episode - sweep through states and actions updating $Q$ as  \n",
    "\n",
    "$Q\\left(s_{k-1},\\,a_{k}\\right)=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,a_{i}\\right)$\n",
    "\n",
    "Now using parameterized representations for the $Q$ function we treat each such step as data producing, and then fit the respective model too it. So to produce the data point - which we denote by $q_k$ - we simply swap out the $Q$ matrix on the right hand side with its respective function as\n",
    "\n",
    "$q_{k}=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,h_i\\left(s_{k};\\,\\theta_i\\right)$\n",
    "\n",
    "Then, letting $i^\\star$ denote the index of the maximum-achieving function on the right and side we re-fit its parameters via Least Squares \n",
    "\n",
    "$\\theta_{i^\\star}\\longleftarrow\\underset{\\theta}{\\text{argmin}}\\,\\left\\Vert h_{i^\\star}\\left(s_{k}~;\\,\\theta\\right)-q_{k}\\right\\Vert _{2}^{2}$\n",
    "\n",
    "The total algorithm then looks like \n",
    "\n",
    "------\n",
    "Initialize $Q$, choose value for $\\gamma \\in [0,1]$\n",
    "\n",
    "**for** e = 1...E (the maximum number of episodes)\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a random initial state $s_0$ and $k=0$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **while** end state OR maximum iteration count NOT reached\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;select a random action $a_k$ at our current state $s_{k-1}$ and move to $s_k$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;update $Q$ as \n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$q_{k}=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,h_i\\left(s_{k};\\,\\theta_i\\right)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$i^{\\star}=\\underset{i=1...M}{\\text{argmax}}\\,\\,h\\left(s_{k};\\,\\theta_{i}\\right)$\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{i^\\star}\\longleftarrow\\underset{\\theta}{\\text{argmin}}\\,\\left\\Vert h_{i^\\star}\\left(s_{k}~;\\,\\theta\\right)-q_{k}\\right\\Vert _{2}^{2}$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\longrightarrow k+1$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4  What is the best action per state given a learned Q function?\n",
    "\n",
    "Given $Q(s_k,a)$ learned properly, how should the agent move at each state?  This is implicit in the recursive definition of $Q$ itself.  Since the action maximizing long term reward maximizing reward at state $s_k$ is given by \n",
    "\n",
    "$\\underset{i=1...M}{\\text{maximum}}\\,\\,h\\left(s_{k};\\,\\theta_{i}\\right)$\n",
    "\n",
    "the action $a_i^\\star$ providing this maximimum is given as the argument of this maximum\n",
    "\n",
    "$i^{\\star}=\\underset{i=1...M}{\\text{argmax}}\\,\\,h\\left(s_{k};\\,\\theta_{i}\\right)$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
