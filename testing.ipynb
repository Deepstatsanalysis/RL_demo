{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from large_gridworld_ipythonblocks import BlockGrid\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class small_gridworld():\n",
    "    \n",
    "    def __init__(self,grid_size):\n",
    "        # use either a small pre-staged grid world or big one with random hazards\n",
    "        \n",
    "        ### initialize grid, agent, obstacles, etc.,\n",
    "        self.width = 5\n",
    "        self.height = 4\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "        \n",
    "        # decide on hazards and goal locations\n",
    "        self.hazards = [[0,3],[1,3],[2,3]]  # impenetrable obstacle locations          \n",
    "        self.goal = [0,4]     # goal block\n",
    "        self.player = [0,0]   # initial location player\n",
    "        \n",
    "        # index states for Q matrix\n",
    "        self.states = []\n",
    "        for i in range(self.grid.height):\n",
    "            for j in range(self.grid.width):\n",
    "                block = [i,j]\n",
    "                self.states.append(str(i) + str(j))\n",
    "        \n",
    "        if grid_size == 'large':\n",
    "            ### initialize grid, agent, obstacles, etc.,\n",
    "            self.width = 20\n",
    "            self.height = 10\n",
    "            self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "            # decide on player and goal locations\n",
    "            self.goal = [0,self.width-1]     # goal block\n",
    "            self.player = [0,0]   # initial location player\n",
    "            \n",
    "            # index states for Q matrix\n",
    "            self.states = []\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    block = [i,j]\n",
    "                    self.states.append(str(i) + str(j))\n",
    "\n",
    "            # decide on hazard locations\n",
    "            num_hazards = 50\n",
    "            self.hazards = []\n",
    "            inds = np.random.permutation(self.grid.width*self.grid.height)\n",
    "            inds = inds[:num_hazards]\n",
    "            k = 0\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    if k in inds: \n",
    "                        block = [i,j]\n",
    "                        if block != self.goal:\n",
    "                            self.hazards.append(block)\n",
    "                    k+=1\n",
    "                                \n",
    "        # initialize action choices\n",
    "        self.action_choices = [[-1,0],[1,0],[0,-1],[0,1]]\n",
    "        \n",
    "        # initialize Q^* matrix\n",
    "        self.Q_star = np.zeros((self.grid.width*self.grid.height,len(self.action_choices)))\n",
    "        \n",
    "    def color_grid(self):                            \n",
    "        # remake + recolor grid\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "        # color obstacles\n",
    "        for i in range(len(self.hazards)):\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].green = 100\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].red = 250\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].blue = 0\n",
    "\n",
    "        # make and color goal\n",
    "        self.grid[self.goal[0],self.goal[1]].green = 255\n",
    "        self.grid[self.goal[0],self.goal[1]].red = 0\n",
    "        self.grid[self.goal[0],self.goal[1]].blue = 0\n",
    "        \n",
    "        # color player location\n",
    "        self.grid[self.player[0],self.player[1]].green = 0\n",
    "        self.grid[self.player[0],self.player[1]].red = 0\n",
    "        self.grid[self.player[0],self.player[1]].blue = 200\n",
    "        \n",
    "        self.grid.show()\n",
    "        \n",
    "    ## Q-learning function\n",
    "    def qlearn(self,gamma,hazard_penalty,num_train_animate):\n",
    "        num_episodes = 300\n",
    "        num_complete = 0\n",
    "        \n",
    "        # loop over episodes, for each run simulation and update Q\n",
    "        for n in range(num_episodes):\n",
    "            # pick random initialization \n",
    "            obstical_free = 0\n",
    "            loc = [np.random.randint(self.grid.height),np.random.randint(self.grid.width)]\n",
    "           \n",
    "            # update Q matrix while loc != goal\n",
    "            steps = 0  # step counter\n",
    "            max_steps = 10*self.grid.width*self.grid.height  # maximum number of steps per episode\n",
    "            while steps < max_steps:    \n",
    "                # when you reach the goal end current episode\n",
    "                if loc == self.goal:\n",
    "                    break\n",
    "                    \n",
    "                ### choose action - left = 0, right = 1, up = 2, down = 3\n",
    "                k = np.random.randint(len(self.action_choices))  \n",
    "                loc2 = [sum(x) for x in zip(loc, self.action_choices[k])] \n",
    "                ind_old = self.states.index(str(loc[0]) + str(loc[1]))\n",
    "\n",
    "                ### set reward    \n",
    "                # is the new location just a regular square?  Than small negative reward\n",
    "                r_k = int(-1)\n",
    "\n",
    "                # if new state is hazard penalize with medium negative value - this needs to be set properly if you're trying to prove a point i.e., that a trained agent won't walk over one of these unless going around costs more\n",
    "                if loc2 in self.hazards:\n",
    "                    r_k = int(hazard_penalty)\n",
    "\n",
    "                # if new state is goal set reward of 0\n",
    "                if loc2 == self.goal:\n",
    "                    r_k = int(0)\n",
    "                    \n",
    "                # if new state is outside of boundaries of grid world penalize set reward to small negative value (like -1) and do not move\n",
    "                if loc2[0] > self.grid.height-1 or loc2[0] < 0 or loc2[1] > self.grid.width-1 or loc2[1] < 0:  \n",
    "                    r_k = int(-1)\n",
    "                    loc2 = loc\n",
    "\n",
    "                ind_new = self.states.index(str(loc2[0]) + str(loc2[1]))\n",
    "                self.Q_star[ind_old,k] = r_k + gamma*max(self.Q_star[ind_new,:])\n",
    "                    \n",
    "                # update current location of player to one we just moved too (or stay still if grid world boundary met)\n",
    "                self.player = loc2\n",
    "                loc = loc2\n",
    "                \n",
    "                # the next few lines just animate the first few steps during the first few episodes\n",
    "                if n < num_train_animate and steps < 100:\n",
    "                    self.color_grid()\n",
    "                    time.sleep(0.1)\n",
    "                    display.clear_output(wait=True)\n",
    "                    \n",
    "                # update counter\n",
    "                steps+=1\n",
    "        \n",
    "            # pause briefly between first few episodes to show user cutoff\n",
    "            if n < num_train_animate:\n",
    "                time.sleep(1)\n",
    "                print 'finished episode, animating next episode'\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                display.clear_output(wait=True)\n",
    "                print 'continuing with remaining episodes without animation...'\n",
    "                \n",
    "        print 'q-learning process complete'\n",
    "                \n",
    "    # print out\n",
    "    def show_qmat(self):        \n",
    "        df = pd.DataFrame(self.Q_star,columns=['up','down','left','right'], index=self.states)\n",
    "        print df.round(3)             \n",
    "            \n",
    "    # animate the testing phase based on completed Q-learning cycle\n",
    "    def animate_testing_phase(self,loc):\n",
    "        # show movement based on an initial \n",
    "        self.player = loc # initial agent location\n",
    "\n",
    "        # if you chose an invalid starting position, break out and try again\n",
    "        if  loc == self.goal or loc[0] > self.grid.height-1 or loc[0] < 0 or loc[1] > self.grid.width-1 or loc[1] < 0:\n",
    "            print 'initialization is outside of gridworld or is goal, try again'\n",
    "        else:  \n",
    "            # now use the learned Q* matrix to run from any (valid) initial point to the goal\n",
    "            self.color_grid()\n",
    "            time.sleep(0.3)\n",
    "            display.clear_output(wait=True)\n",
    "            count = 0\n",
    "            max_count = self.grid.width*self.grid.height\n",
    "            while count < max_count:\n",
    "                # find next state using max Q* value\n",
    "                ind_old = self.states.index(str(self.player[0]) + str(self.player[1]))\n",
    "                \n",
    "                # find biggest value in Q* and determine block location\n",
    "                action_ind = np.argmax(self.Q_star[ind_old,:])\n",
    "                action = self.action_choices[action_ind]\n",
    "                \n",
    "                # move player to new location and recolor - if you move out of gridworld halt and report this to user\n",
    "                new_loc = [sum(x) for x in zip(self.player,action)] \n",
    "                \n",
    "                if new_loc[0] > self.grid.height-1 or new_loc[0] < 0 or new_loc[1] > self.grid.width-1 or new_loc[1] < 0:\n",
    "                    print 'something went wrong - the player has left the gridworld arena'\n",
    "                    print \"your episodes did not the states reached by your initialization enough to train Q properly here\"\n",
    "                    print \"this is likely because you didn't traiin long enough - up the number of steps / episodes and try again\"\n",
    "                    \n",
    "                self.player = new_loc\n",
    "                \n",
    "                # clear current screen for next step\n",
    "                self.color_grid()\n",
    "                time.sleep(0.2)\n",
    "                if self.player == self.goal:\n",
    "                    break\n",
    "                display.clear_output(wait=True)\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing with remaining episodes without animation...\n",
      "q-learning process complete\n"
     ]
    }
   ],
   "source": [
    "# create instance of grid world\n",
    "test = small_gridworld(grid_size = 'large')\n",
    "\n",
    "# train q learning and lets examine the matrix as it develops\n",
    "test.qlearn(gamma = 0.8,hazard_penalty = -4,num_train_animate = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(0, 0, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run testing phase\n",
    "starting_location = [0,0]\n",
    "test.animate_testing_phase(starting_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       up   down   left  right\n",
      "00 -4.463 -4.329 -4.463 -4.329\n",
      "01 -4.329 -4.161 -4.463 -4.161\n",
      "02 -4.161 -3.951 -4.329 -4.000\n",
      "03 -1.000 -4.800 -4.161  0.000\n",
      "04  0.000  0.000  0.000  0.000\n",
      "10 -4.463 -4.161 -4.329 -4.161\n",
      "11 -4.329 -3.951 -4.329 -3.951\n",
      "12 -4.161 -3.689 -4.161 -4.800\n",
      "13 -4.000 -5.440 -3.951 -1.000\n",
      "14  0.000 -1.800 -4.800 -1.000\n",
      "20 -4.329 -3.951 -4.161 -3.951\n",
      "21 -4.161 -3.689 -4.161 -3.689\n",
      "22 -3.951 -3.362 -3.951 -5.440\n",
      "23 -4.800 -2.952 -3.689 -1.800\n",
      "24 -1.000 -2.440 -5.440 -1.800\n",
      "30 -4.161 -3.951 -3.951 -3.689\n",
      "31 -3.951 -3.689 -3.951 -3.362\n",
      "32 -3.689 -3.362 -3.689 -2.952\n",
      "33 -5.440 -2.952 -3.362 -2.440\n",
      "34 -1.800 -2.440 -2.952 -2.440\n"
     ]
    }
   ],
   "source": [
    "test.show_qmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cart pole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-02 07:48:09,018] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01113717  0.00537443  0.01484656 -0.02708079]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(1):\n",
    "#         env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# always begin at the same state\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions\n",
    "\n",
    "0 - move to the right\n",
    "\n",
    "1 - move to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03252785, -0.03166994, -0.02746844, -0.00841382])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes: \n",
    "- the cart position -> the 1st entry of the state\n",
    "\n",
    "- the cart velocity -> the 2nd entry of the state\n",
    "\n",
    "- the angle of the pole measured as its deviation from the vertical position - 3rd entry of state (in RADIANS)\n",
    "\n",
    "- and the angular velocity of the pole - 4th entry of state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-02 07:18:25,508] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)                # 0,1\n",
    "print(env.observation_space)           # 4 continuous values\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(env.observation_space)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028863538205226077"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(observation,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class my_cartpole():\n",
    "\n",
    "    def __init__(self):\n",
    "        # initialize enviroment\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "                                \n",
    "        # initialize action choices\n",
    "        self.action_choices = [0,1]\n",
    "        \n",
    "        # initialize Q matrix parameters\n",
    "        self.Q = np.zeros((np.shape(env.observation_space)[0]+1,env.action_space.n))\n",
    "        \n",
    "    ## Q-learning function\n",
    "    def qlearn(self,gamma):\n",
    "        num_episodes = 200\n",
    "        num_complete = 0\n",
    "        \n",
    "        # loop over episodes, for each run simulation and update Q\n",
    "        for n in range(num_episodes):\n",
    "            # we use the same initialization every time\n",
    "            observation = env.reset()  # note here 'observation' = state\n",
    "            \n",
    "            # discretize, i.e., round observation features\n",
    "            observation = np.around(observation,3)\n",
    "\n",
    "            '''\n",
    "             Note here that there are four features to the given state:\n",
    "            - the cart position -> the 1st entry of the state\n",
    "            - the cart velocity -> the 2nd entry of the state\n",
    "            - the angle of the pole measured as its deviation from the vertical position - 3rd entry of state (in radians)\n",
    "            - the angular velocity of the pole - 4th entry of state\n",
    "            '''\n",
    "            \n",
    "            # start episode of simulation training\n",
    "            steps = 0\n",
    "            max_steps = 500\n",
    "            while steps < max_steps:\n",
    "                # take random action - either a 0 or 1\n",
    "                action = env.action_space.sample()\n",
    "                observation, reward, done, info = env.step(action)  # reward = +1 for every time unit the pole is above a threshold angle, 0 else\n",
    "\n",
    "                # discretize, i.e., round observation features\n",
    "                observation = np.around(observation,3)\n",
    "                \n",
    "                # compute long term reward based on proper linear model from Q\n",
    "                state = np.insert(observation,0,1)\n",
    "                h_i = np.dot(self.Q[:,action],state)\n",
    "                \n",
    "                # compute Q cost value\n",
    "                q_k = reward + gamma*h_i\n",
    "                \n",
    "                # solve linear system to update paramters\n",
    "                state.shape = (len(state),1)\n",
    "                theta_i = np.linalg.pinv(state)*q_k\n",
    "                self.Q[:,action] = theta_i.ravel()\n",
    "                \n",
    "                # if pole goes below threshold angle restart - new episode\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # update counter\n",
    "                steps+=1\n",
    "                \n",
    "        print 'q-learning process complete'      \n",
    "    \n",
    "    def animate_testing_phase(self):\n",
    "        # start up simulation episode\n",
    "        observation = env.reset()  # note here 'observation' = state\n",
    "        \n",
    "        # discretize, i.e., round observation features\n",
    "        observation = np.around(observation,3)\n",
    "        \n",
    "        # start testing phase\n",
    "        steps = 0\n",
    "        max_steps = 500\n",
    "        while steps < max_steps:\n",
    "            state = np.insert(observation,0,1)\n",
    "            state.shape = (len(state),1)\n",
    "\n",
    "            # pick best action based on input state\n",
    "            h = np.dot(self.Q.T,state)\n",
    "            action = int(np.argmax(h))\n",
    "            \n",
    "            # take action \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # discretize, i.e., round observation features\n",
    "            observation = np.around(observation,3)\n",
    "        \n",
    "            # render action in animation\n",
    "            # env.render()\n",
    "            \n",
    "            # if pole goes below threshold then end simulation\n",
    "            if done:\n",
    "                print(\"lasted {} timesteps\".format(steps))\n",
    "                break\n",
    "                \n",
    "            steps+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-02 12:38:19,376] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning process complete\n"
     ]
    }
   ],
   "source": [
    "test = my_cartpole()\n",
    "test.qlearn(gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasted 8 timesteps\n"
     ]
    }
   ],
   "source": [
    "test.animate_testing_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95771043,  1.02567891],\n",
       "       [ 0.1924998 ,  0.18257085],\n",
       "       [ 0.9548373 ,  1.21953223],\n",
       "       [-0.22410424, -0.20103307],\n",
       "       [-1.58884161, -1.93340475]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = test.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.12,  2.7 ],\n",
       "       [ 0.23,  0.2 ],\n",
       "       [-0.47,  0.11],\n",
       "       [-0.67, -0.53],\n",
       "       [-1.93, -2.28]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
