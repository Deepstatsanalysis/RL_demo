{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from large_gridworld_ipythonblocks import BlockGrid\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class small_gridworld():\n",
    "    \n",
    "    def __init__(self,grid_size):\n",
    "        # use either a small pre-staged grid world or big one with random hazards\n",
    "        \n",
    "        ### initialize grid, agent, obstacles, etc.,\n",
    "        self.width = 5\n",
    "        self.height = 4\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "        \n",
    "        # decide on hazards and goal locations\n",
    "        self.hazards = [[0,3],[1,3],[2,3]]  # impenetrable obstacle locations          \n",
    "        self.goal = [0,4]     # goal block\n",
    "        self.player = [0,0]   # initial location player\n",
    "        \n",
    "        # index states for Q matrix\n",
    "        self.states = []\n",
    "        for i in range(self.grid.height):\n",
    "            for j in range(self.grid.width):\n",
    "                block = [i,j]\n",
    "                self.states.append(str(i) + str(j))\n",
    "        \n",
    "        if grid_size == 'large':\n",
    "            ### initialize grid, agent, obstacles, etc.,\n",
    "            self.width = 20\n",
    "            self.height = 10\n",
    "            self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "            # decide on player and goal locations\n",
    "            self.goal = [0,self.width-1]     # goal block\n",
    "            self.player = [0,0]   # initial location player\n",
    "            \n",
    "            # index states for Q matrix\n",
    "            self.states = []\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    block = [i,j]\n",
    "                    self.states.append(str(i) + str(j))\n",
    "\n",
    "            # decide on hazard locations\n",
    "            num_hazards = 50\n",
    "            self.hazards = []\n",
    "            inds = np.random.permutation(self.grid.width*self.grid.height)\n",
    "            inds = inds[:num_hazards]\n",
    "            k = 0\n",
    "            for i in range(self.grid.height):\n",
    "                for j in range(self.grid.width):\n",
    "                    if k in inds: \n",
    "                        block = [i,j]\n",
    "                        if block != self.goal:\n",
    "                            self.hazards.append(block)\n",
    "                    k+=1\n",
    "                                \n",
    "        # initialize action choices\n",
    "        self.action_choices = [[-1,0],[1,0],[0,-1],[0,1]]\n",
    "        \n",
    "        # initialize Q^* matrix\n",
    "        self.Q_star = np.zeros((self.grid.width*self.grid.height,len(self.action_choices)))\n",
    "        \n",
    "    def color_grid(self):                            \n",
    "        # remake + recolor grid\n",
    "        self.grid = BlockGrid(self.width,self.height, fill=(200, 200, 200))\n",
    "\n",
    "        # color obstacles\n",
    "        for i in range(len(self.hazards)):\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].green = 100\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].red = 250\n",
    "            self.grid[self.hazards[i][0],self.hazards[i][1]].blue = 0\n",
    "\n",
    "        # make and color goal\n",
    "        self.grid[self.goal[0],self.goal[1]].green = 255\n",
    "        self.grid[self.goal[0],self.goal[1]].red = 0\n",
    "        self.grid[self.goal[0],self.goal[1]].blue = 0\n",
    "        \n",
    "        # color player location\n",
    "        self.grid[self.player[0],self.player[1]].green = 0\n",
    "        self.grid[self.player[0],self.player[1]].red = 0\n",
    "        self.grid[self.player[0],self.player[1]].blue = 200\n",
    "        \n",
    "        self.grid.show()\n",
    "        \n",
    "    ## Q-learning function\n",
    "    def qlearn(self,gamma,hazard_penalty,num_train_animate):\n",
    "        num_episodes = 300\n",
    "        num_complete = 0\n",
    "        \n",
    "        # loop over episodes, for each run simulation and update Q\n",
    "        for n in range(num_episodes):\n",
    "            # pick random initialization \n",
    "            obstical_free = 0\n",
    "            loc = [np.random.randint(self.grid.height),np.random.randint(self.grid.width)]\n",
    "           \n",
    "            # update Q matrix while loc != goal\n",
    "            steps = 0  # step counter\n",
    "            max_steps = 10*self.grid.width*self.grid.height  # maximum number of steps per episode\n",
    "            while steps < max_steps:    \n",
    "                # when you reach the goal end current episode\n",
    "                if loc == self.goal:\n",
    "                    break\n",
    "                    \n",
    "                ### choose action - left = 0, right = 1, up = 2, down = 3\n",
    "                k = np.random.randint(len(self.action_choices))  \n",
    "                loc2 = [sum(x) for x in zip(loc, self.action_choices[k])] \n",
    "                ind_old = self.states.index(str(loc[0]) + str(loc[1]))\n",
    "\n",
    "                ### set reward    \n",
    "                # is the new location just a regular square?  Than small negative reward\n",
    "                r_k = int(-1)\n",
    "\n",
    "                # if new state is hazard penalize with medium negative value - this needs to be set properly if you're trying to prove a point i.e., that a trained agent won't walk over one of these unless going around costs more\n",
    "                if loc2 in self.hazards:\n",
    "                    r_k = int(hazard_penalty)\n",
    "\n",
    "                # if new state is goal set reward of 0\n",
    "                if loc2 == self.goal:\n",
    "                    r_k = int(0)\n",
    "                    \n",
    "                # if new state is outside of boundaries of grid world penalize set reward to small negative value (like -1) and do not move\n",
    "                if loc2[0] > self.grid.height-1 or loc2[0] < 0 or loc2[1] > self.grid.width-1 or loc2[1] < 0:  \n",
    "                    r_k = int(-1)\n",
    "                    loc2 = loc\n",
    "\n",
    "                ind_new = self.states.index(str(loc2[0]) + str(loc2[1]))\n",
    "                self.Q_star[ind_old,k] = r_k + gamma*max(self.Q_star[ind_new,:])\n",
    "                    \n",
    "                # update current location of player to one we just moved too (or stay still if grid world boundary met)\n",
    "                self.player = loc2\n",
    "                loc = loc2\n",
    "                \n",
    "                # the next few lines just animate the first few steps during the first few episodes\n",
    "                if n < num_train_animate and steps < 100:\n",
    "                    self.color_grid()\n",
    "                    time.sleep(0.1)\n",
    "                    display.clear_output(wait=True)\n",
    "                    \n",
    "                # update counter\n",
    "                steps+=1\n",
    "        \n",
    "            # pause briefly between first few episodes to show user cutoff\n",
    "            if n < num_train_animate:\n",
    "                time.sleep(1)\n",
    "                print 'finished episode, animating next episode'\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                display.clear_output(wait=True)\n",
    "                print 'continuing with remaining episodes without animation...'\n",
    "                \n",
    "        print 'q-learning process complete'\n",
    "                \n",
    "    # print out\n",
    "    def show_qmat(self):        \n",
    "        df = pd.DataFrame(self.Q_star,columns=['up','down','left','right'], index=self.states)\n",
    "        print df.round(3)             \n",
    "            \n",
    "    # animate the testing phase based on completed Q-learning cycle\n",
    "    def animate_testing_phase(self,loc):\n",
    "        # show movement based on an initial \n",
    "        self.player = loc # initial agent location\n",
    "\n",
    "        # if you chose an invalid starting position, break out and try again\n",
    "        if  loc == self.goal or loc[0] > self.grid.height-1 or loc[0] < 0 or loc[1] > self.grid.width-1 or loc[1] < 0:\n",
    "            print 'initialization is outside of gridworld or is goal, try again'\n",
    "        else:  \n",
    "            # now use the learned Q* matrix to run from any (valid) initial point to the goal\n",
    "            self.color_grid()\n",
    "            time.sleep(0.3)\n",
    "            display.clear_output(wait=True)\n",
    "            count = 0\n",
    "            max_count = self.grid.width*self.grid.height\n",
    "            while count < max_count:\n",
    "                # find next state using max Q* value\n",
    "                ind_old = self.states.index(str(self.player[0]) + str(self.player[1]))\n",
    "                \n",
    "                # find biggest value in Q* and determine block location\n",
    "                action_ind = np.argmax(self.Q_star[ind_old,:])\n",
    "                action = self.action_choices[action_ind]\n",
    "                \n",
    "                # move player to new location and recolor - if you move out of gridworld halt and report this to user\n",
    "                new_loc = [sum(x) for x in zip(self.player,action)] \n",
    "                \n",
    "                if new_loc[0] > self.grid.height-1 or new_loc[0] < 0 or new_loc[1] > self.grid.width-1 or new_loc[1] < 0:\n",
    "                    print 'something went wrong - the player has left the gridworld arena'\n",
    "                    print \"your episodes did not the states reached by your initialization enough to train Q properly here\"\n",
    "                    print \"this is likely because you didn't traiin long enough - up the number of steps / episodes and try again\"\n",
    "                    \n",
    "                self.player = new_loc\n",
    "                \n",
    "                # clear current screen for next step\n",
    "                self.color_grid()\n",
    "                time.sleep(0.2)\n",
    "                if self.player == self.goal:\n",
    "                    break\n",
    "                display.clear_output(wait=True)\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing with remaining episodes without animation...\n",
      "q-learning process complete\n"
     ]
    }
   ],
   "source": [
    "# create instance of grid world\n",
    "test = small_gridworld(grid_size = 'large')\n",
    "\n",
    "# train q learning and lets examine the matrix as it develops\n",
    "test.qlearn(gamma = 0.8,hazard_penalty = -4,num_train_animate = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(0, 0, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(250, 100, 0);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr><tr><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td><td style=\"width: 25px; height: 25px; border: 3px solid white; background-color: rgb(200, 200, 200);\"></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run testing phase\n",
    "starting_location = [0,0]\n",
    "test.animate_testing_phase(starting_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       up   down   left  right\n",
      "00 -4.463 -4.329 -4.463 -4.329\n",
      "01 -4.329 -4.161 -4.463 -4.161\n",
      "02 -4.161 -3.951 -4.329 -4.000\n",
      "03 -1.000 -4.800 -4.161  0.000\n",
      "04  0.000  0.000  0.000  0.000\n",
      "10 -4.463 -4.161 -4.329 -4.161\n",
      "11 -4.329 -3.951 -4.329 -3.951\n",
      "12 -4.161 -3.689 -4.161 -4.800\n",
      "13 -4.000 -5.440 -3.951 -1.000\n",
      "14  0.000 -1.800 -4.800 -1.000\n",
      "20 -4.329 -3.951 -4.161 -3.951\n",
      "21 -4.161 -3.689 -4.161 -3.689\n",
      "22 -3.951 -3.362 -3.951 -5.440\n",
      "23 -4.800 -2.952 -3.689 -1.800\n",
      "24 -1.000 -2.440 -5.440 -1.800\n",
      "30 -4.161 -3.951 -3.951 -3.689\n",
      "31 -3.951 -3.689 -3.951 -3.362\n",
      "32 -3.689 -3.362 -3.689 -2.952\n",
      "33 -5.440 -2.952 -3.362 -2.440\n",
      "34 -1.800 -2.440 -2.952 -2.440\n"
     ]
    }
   ],
   "source": [
    "test.show_qmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
